.. SAR documentation master file, created by
   sphinx-quickstart on Mon Mar 28 07:30:12 2022.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.


Welcome to SAR's documentation!
===============================

   
SAR is a pure Python library built on top of  `DGL <https://www.dgl.ai/>`_ to accelerate distributed training of Graph Neural Networks (GNNs) on large graphs. SAR supports distributed full-batch training, distributed sampling-based training and single-node training on very large graphs.

For full-batch training, SAR supports the `Sequenial Aggregation and Rematerialization (SAR)  <https://proceedings.mlsys.org/paper_files/paper/2022/hash/1d781258d409a6efc66cd1aa14a1681c-Abstract.html>`_  scheme to reduce peak per-machine memory consumption and guarantee that model memory consumption per worker goes down linearly with the number of workers.  This is achieved by eliminating most of the data redundancy (due to the halo effect) involved in standard spatially parallel training. 

SAR uses the graph partition data generated by DGL's `partitioning utilities <https://docs.dgl.ai/en/0.6.x/generated/dgl.distributed.partition.partition_graph.html>`_. It can thus be used as a drop in replacement for DGL's sampling-based distributed training. SAR enables scalable, distributed training on very large graphs, and supports multiple training modes that balance speed against memory efficiency. SAR requires minimal changes  to existing single-host DGL training code.

SAR also enables single-node training, which allows full graph training/inference to be performed on very large graphs on a single machine, which in a normal scenario could trigger out-of-memory exception.

See the quick start guide to get started using SAR.

.. toctree::
   :maxdepth: 1

   Quick start<quick_start>
   Data loading and graph construction <data_loading>
   Communication routines<comm>
   Full-batch training<full_batch>
   Sampling-based training<sampling_training>
   Single-node training<single_node>
   Data tuples<common_tuples>
   SAR Configuration<sar_config>
   Environment variables<environment_variables>

Index
==================

* :ref:`genindex`
